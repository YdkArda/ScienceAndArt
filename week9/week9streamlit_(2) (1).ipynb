{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s7V8zkK9uxnQ",
        "outputId": "86220ad8-a583-49e4-c4a8-2dcfc0a99572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Downloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, pydeck, bitsandbytes, streamlit\n",
            "Successfully installed bitsandbytes-0.49.0 pydeck-0.9.1 pyngrok-7.5.0 streamlit-1.52.1\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit  bitsandbytes accelerate pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gfMa0utRTwN"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/.streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSJ_tlBcNgqi"
      },
      "outputs": [],
      "source": [
        "!echo 'HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"' > /content/.streamlit/secrets.toml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4GMI60HOvA2",
        "outputId": "c39fe9fd-809c-4b27-c2d8-e98e82f6634f"
      },
      "outputs": [],
      "source": [
        "!cat /content/.streamlit/secrets.toml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgA_KRoKugJr",
        "outputId": "b734a87a-01d4-4f36-dbff-371270609e53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import streamlit as st\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = st.secrets[\"HF_TOKEN\"]\n",
        "HUGGINGFACE_TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from huggingface_hub import InferenceClient\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "\n",
        "if not HUGGINGFACE_TOKEN:\n",
        "    # EÄŸer token hala bulunamÄ±yorsa, kullanÄ±cÄ±ya uyarÄ± gÃ¶ster ve uygulamayÄ± durdur.\n",
        "    # Bu, LLM veya GÃ¶rÃ¼ntÃ¼ Ã¼retme kodunun Ã§alÄ±ÅŸmasÄ±nÄ± engeller.\n",
        "    st.error(\" Hata: Hugging Face API Token (HF_TOKEN) bulunamadÄ±.\")\n",
        "\n",
        "    st.stop() # Token yoksa uygulamayÄ± burada durdur.\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def load_mistral_model():\n",
        "    \"\"\"\n",
        "    Loads the Mistral 7B Instruct model using 8-bit quantization.\n",
        "    \"\"\"\n",
        "    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "    # Colab GPU'su ile Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olmak iÃ§in\n",
        "    # Colab menÃ¼sÃ¼nden Ã‡alÄ±ÅŸma ZamanÄ± -> Ã‡alÄ±ÅŸma ZamanÄ± TÃ¼rÃ¼nÃ¼ DeÄŸiÅŸtir -> T4 GPU seÃ§ili olmalÄ±!\n",
        "\n",
        "    with st.spinner(f\"Yapay Zeka Modeli YÃ¼kleniyor... ({model_id} - 8-bit Nicemleme)\"):\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            load_in_8bit=True, # Sizin Ã§alÄ±ÅŸan versiyonunuzda 8-bit yÃ¼kleme vardÄ±.\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model.eval()\n",
        "\n",
        "    st.success(\"Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# --- 2. Generation Function ---\n",
        "def generate_response(model, tokenizer, prompt, max_len=1000):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # GiriÅŸ token'larÄ±nÄ± modelin bulunduÄŸu cihaza (GPU) taÅŸÄ±\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **tokens,\n",
        "            max_new_tokens=max_len,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            num_beams=3,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full_output = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    if full_output.startswith(prompt):\n",
        "        response = full_output[len(prompt):].strip()\n",
        "    else:\n",
        "        response = full_output.strip()\n",
        "\n",
        "    end_time = time.time()\n",
        "    st.info(f\"YanÄ±t {end_time - start_time:.2f} saniyede Ã¼retildi.\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# --- 3. Streamlit UI (Main App) ---\n",
        "markdown = st.markdown(\"### SELECT YOUR AI MODE\")\n",
        "selected_mode = st.selectbox(\"\", [\"text to text\", \"text to image\"])\n",
        "\n",
        "if selected_mode == \"text to text\":\n",
        "    # ------------------ TEXT-TO-TEXT LOGIC ------------------\n",
        "    model, tokenizer = load_mistral_model()\n",
        "    st.title(\"ðŸ¤– Mistral 7B Prompt YanÄ±tlama UygulamasÄ±\")\n",
        "    st.markdown(\"8-bit nicemleme (quantization) ile T4 GPU'da Ã§alÄ±ÅŸan bÃ¼yÃ¼k dil modeli.\")\n",
        "\n",
        "    user_prompt = st.text_area(\n",
        "        \"LÃ¼tfen metin isteminizi (prompt) girin:\",\n",
        "        value=\"BÃ¼yÃ¼k dil modellerinin geleceÄŸi hakkÄ±nda bir Ã¶zet yaz.\",\n",
        "        height=200\n",
        "    )\n",
        "\n",
        "    if st.button(\"YanÄ±t Ãœret\", type=\"primary\"):\n",
        "        if user_prompt:\n",
        "            with st.status(\"YanÄ±t Ãœretiliyor...\", expanded=True) as status:\n",
        "                st.write(\"Model Ã§Ä±karÄ±m yapÄ±yor...\")\n",
        "                try:\n",
        "                    ai_response = generate_response(model, tokenizer, user_prompt)\n",
        "                    status.update(label=\"YanÄ±t Ãœretimi TamamlandÄ±!\", state=\"complete\", expanded=False)\n",
        "                    st.subheader(\"Mistral 7B YanÄ±tÄ±:\")\n",
        "                    st.markdown(ai_response)\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Bir hata oluÅŸtu: {e}\")\n",
        "                    status.update(label=\"Hata!\", state=\"error\")\n",
        "        else:\n",
        "            st.warning(\"LÃ¼tfen geÃ§erli bir prompt girin.\")\n",
        "\n",
        "else: # selected_mode == \"text to image\"\n",
        "    # ------------------ TEXT-TO-IMAGE LOGIC ------------------\n",
        "    st.title(\"ðŸ–¼ï¸ GÃ¶rÃ¼ntÃ¼ Ãœretme UygulamasÄ± (Hugging Face API)\")\n",
        "    st.markdown(\"Stable Diffusion XL (SDXL) modelini kullanarak metinden gÃ¶rÃ¼ntÃ¼ Ã¼retin.\")\n",
        "\n",
        "    # Token'Ä±n geÃ§erliliÄŸini zaten yukarÄ±da kontrol ettik. Burada istemciyi baÅŸlatabiliriz.\n",
        "    MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "    try:\n",
        "        client = InferenceClient(token=HUGGINGFACE_TOKEN)\n",
        "    except Exception as e:\n",
        "        st.error(f\"InferenceClient baÅŸlatÄ±lamadÄ±: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "    # 3. Get the prompt from user\n",
        "    prompt = st.text_area(\n",
        "        \"LÃ¼tfen Ã¼retmek istediÄŸiniz gÃ¶rÃ¼ntÃ¼nÃ¼n Ä°ngilizce tanÄ±mÄ±nÄ± (prompt) girin:\",\n",
        "        value=\"A hyper-realistic cyborg dog wearing a space helmet, digital art, high contrast\",\n",
        "        height=200\n",
        "    )\n",
        "\n",
        "    negative_prompt = st.text_input(\n",
        "        \"Negatif Prompt (Ä°stenmeyen Ã–zellikler):\",\n",
        "        value=\"blurry, ugly, poorly drawn, extra limbs, watermark\",\n",
        "    )\n",
        "\n",
        "    # 4. Handle Button Click\n",
        "    if st.button(\"GÃ¶rÃ¼ntÃ¼ Ãœret\", type=\"primary\"):\n",
        "        if not prompt:\n",
        "            st.warning(\"LÃ¼tfen geÃ§erli bir prompt girin.\")\n",
        "            st.stop()\n",
        "\n",
        "        with st.status(\"GÃ¶rÃ¼ntÃ¼ Ãœretiliyor...\", expanded=True) as status:\n",
        "            st.write(f\"Prompt: {prompt}\")\n",
        "\n",
        "            try:\n",
        "                # 4. Call the free Inference API\n",
        "                status.write(\"Hugging Face API'sine istek gÃ¶nderiliyor. Bu iÅŸlem 30-60 saniye sÃ¼rebilir...\")\n",
        "\n",
        "                image = client.text_to_image(\n",
        "                    model=MODEL_ID,\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    guidance_scale=9,\n",
        "                    width=1024,\n",
        "                    height=1024\n",
        "                )\n",
        "\n",
        "                status.write(\"GÃ¶rÃ¼ntÃ¼ indiriliyor...\")\n",
        "\n",
        "                # 5. Display the image\n",
        "                st.subheader(\"Ãœretilen GÃ¶rÃ¼ntÃ¼:\")\n",
        "                # DoÄŸrudan PIL Image objesini st.image'a iletiyoruz.\n",
        "                st.image(\n",
        "                    image,\n",
        "                    caption=prompt,\n",
        "                    use_column_width=True\n",
        "                )\n",
        "\n",
        "                status.update(label=\"GÃ¶rÃ¼ntÃ¼ BaÅŸarÄ±yla Ãœretildi!\", state=\"complete\", expanded=False)\n",
        "\n",
        "            except requests.HTTPError as e:\n",
        "                if \"rate limit\" in str(e):\n",
        "                    st.error(\"**Hata:** Ãœcretsiz hÄ±z limitine ulaÅŸtÄ±nÄ±z. LÃ¼tfen bir sÃ¼re sonra tekrar deneyin.\")\n",
        "                elif \"Repository Not Found\" in str(e):\n",
        "                    st.error(\" API HatasÄ±: Model ID'si hatalÄ± veya lisansÄ± kabul edilmemiÅŸ. Hugging Face'de lisansÄ± kabul edin.\")\n",
        "                else:\n",
        "                    st.error(f\" Beklenmedik API HatasÄ±: {e}\")\n",
        "                status.update(label=\"Hata!\", state=\"error\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Beklenmedik bir hata oluÅŸtu: {e}\")\n",
        "                status.update(label=\"Hata!\", state=\"error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOxRk0KPwbsy",
        "outputId": "814cb48f-7285-4122-d9e4-63ba885f5f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UygulamanÄ±zÄ±n Genel URL'si: NgrokTunnel: \"https://semiannual-vida-livelily.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN_HERE\")\n",
        "# Streamlit uygulamasÄ±nÄ± arka planda baÅŸlat\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "# Streamlit'in Ã§alÄ±ÅŸtÄ±ÄŸÄ± 8501 portuna tÃ¼nel oluÅŸtur\n",
        "public_url = ngrok.connect('8501')\n",
        "\n",
        "# URL'yi yazdÄ±r\n",
        "print(f\"UygulamanÄ±zÄ±n Genel URL'si: {public_url}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
